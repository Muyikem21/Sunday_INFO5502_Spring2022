{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muyikem21/Sunday_INFO5502_Spring2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyHm2xnCXAWp"
      },
      "source": [
        "## The third Lab-assignment (02/10/2022, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skpEP4eJXAWz"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLUOrLsaXAW0"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNy7G1MtXAW1"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Have you ever wondered how media platforms, and others recommend you what to watch next? To do so, they use a tool called\n",
        "the recommender/recommendation system. It takes several metrics into consideration, such as age, previously watched shows, \n",
        "most-watched genre, watch frequency, and feeds them into a Machine Learning model which then generates what the user might \n",
        "like to watch next. Based on my preference data, I can build either a content-based recommendation system or a collaborative\n",
        "filtering recommendation system. I will need 58,000 movies for the project. To obtain this data, I will use SP500 Index \n",
        "components, whose list is readily available online at the relevant Wikipedia page, and then save the dataset into csv file.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRikJgmIXAW3"
      },
      "source": [
        "Question 2 (10 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ixc7JJ-XAW4"
      },
      "outputs": [],
      "source": [
        "# Import relevant Packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url=\"www.fastquicksearch.com/Movies Movies/Here\"\n",
        "\n",
        "#Scrape data\n",
        "r = requests.get(url,timeout=2.5)\n",
        "r_www = r.text\n",
        "soup = BeautifulSoup(r_www, 'www.parser')\n",
        "components_table = soup.find_all(id=\"constituents\")\n",
        "headers_www = soup.find_all(\"movies\")\n",
        "df_columns=[item.text.rstrip(\"\\n\") for item in headers_www]\n",
        "\n",
        "#Store data into a Pandas Dataframe\n",
        "components_headers=df_columns[:10]\n",
        "data_rows=components_table[0].find(\"movies\").find_all(\"movies\")[1:]\n",
        "rows=[]\n",
        "for row in range(len(data_rows))\n",
        "Mov=list(filter(None,data_rows[row].text.split(\"\\n\")))\n",
        "rows.append(Mov)\n",
        "\n",
        "# Save dataset into csv file\n",
        "S_P_1000_movies=pd.DataFrame(rows,columns=components_headers)\n",
        "S_P_1000_movies.drop(\"SEC filings\",inplace=True,axis=1)\n",
        "S_P_1000_Mov.to_csv(r\"/home/edo_romani1/my-python-directory/SP1000Mov.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mallfXlyXAW5"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG6xvNvMXAW6"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Quality problem of the dataset on https://www.kaggle.com/srikanthladda/house-price-prediction.  \n",
        "1. Incopleteness: Some rows are missing data.\n",
        "2. Timeliness: There has been inflation between the year under study and the present time;\n",
        "   the data is not updated frequently.\n",
        "3. Uniqueness: There are records that exist more than once within the data set. \n",
        "Strategy to clean the data\n",
        "1. The average of the values recorded in each variable (column) will replace the missing values\n",
        "2. There will be inflation adjustment in the sale price using AdjSalePrice= SalePrice*(1+avg_inflation_rate) ^number_yrs_since_purchase  \n",
        "   The average inflation rate for Australia from (2006-2022) is 3.5%. \n",
        "3. Records that exist more than once will be deleted manually. Only one record of such variable will be retained.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p15VlypcXAW7"
      },
      "source": [
        "Question 4 (20 points). Data cleaning: There are two datasets TwADR-L (from Twitter) and AskAPatient (Link: https://zenodo.org/record/55013#.YgU2NN-ZO4T) for medical concept\n",
        "normalization. However, the two datasets have serious data quality problems. Please read the introduction of the datasets and clean the two datasets by following the steps in the statement.\n",
        "\n",
        "In the original dataset, the TwADR-L had 48,057 training, 1,256 validation and 1,427 test examples. The test set (all\n",
        "test samples from 10 folds combined) consists of 765 unique phrases and 273 unique classes (medical concepts). The AskAPatient dataset contained 156,652 training, 7,926 validation, and 8,662 test examples. The entire test set (all test samples\n",
        "from 10 folds combined) consists of 3,749 unique phrases and 1,035 unique classes (medical concepts). The authors\n",
        "randomly split each dataset into ten equal folds, ran 10-fold cross validation and reported the accuracy averaged across the\n",
        "ten folds. \n",
        "\n",
        "We found that, in the original data set, many phrase-label pairs appeared multiple times within the same training data file\n",
        "and also across the training and test data sets in the same fold. In the AskAPatient data set, on average 35.82% of the test data overlapped with training data in the same fold. In the Twitter (TwADR-L) dataset, on average 8.62% of the test set had an overlap with the training data in the same fold. Having a large overlap between the training and the test data can potentially\n",
        "introduce bias in the model and contribute to high accuracy. It is not unlikely that the high model performance reported in the original paper may be triggered by the the large overlap between the training and test sets.\n",
        "\n",
        "Therefore to remove the bias, we further cleaned and recreated the training, validation, and test sets such that each\n",
        "phrase-label pair appears only once in the entire dataset (either in training, validation or test set).\n",
        "\n",
        "(1) First, we combined all examples in training, validation and test data from the original data set and then removed all\n",
        "duplicate phrase-label pairs (examples that have the same phrase and label pair and appear more than once in training/validation/test datasets). Table II shows the statistics of the new dataset (after removing duplicates). The Twitter data set had 3,157 unique phrase-label pairs and 2,220 unique labels (medical concepts) while 173 phrases had multiple labels (i.e., they were assigned to more than one label). Many concepts had only one example, and the concept that had the most number\n",
        "of examples had 36 phrases. On average, each concept had 1.42 examples. The AskAPatient data set had 4,496 unique phrase-label pairs, 1,036 unique labels while 26 phrases had multiple labels. Table III shows examples of phrases that had multiple labels. For example, ‘mad’ can be mapped to ‘anger’ or ‘rage’ and ‘sore’ can be mapped to ‘pain’ or ‘myalgia’.\n",
        "\n",
        "(2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
        "\n",
        "(3) Third, we divide all examples without multiple labels into random 10 folds such that each unique phrase-label pair\n",
        "appears once in one of the 10 test sets. We add the pairs with multiple labels into the training data. This final 10-folds\n",
        "dataset is used in all our experiments.\n",
        "\n",
        "(The original paper can be download on canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDnv5AKeXAW9"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import glob\n",
        "import numpy\n",
        "\n",
        "#1 The data meant for individual files are combined below\n",
        "\n",
        "# twADR-L dataset\n",
        "twitter_df= [];\n",
        "\n",
        "twitter_data_files= glob.glob('/content/TwADR-L/*.txt')\n",
        "\n",
        "for file in twitter_data_files:\n",
        "  twitter_df.append(pandas.read_csv(file, sep='\\t',header=None,index_col=None))\n",
        "\n",
        "twitter_df=pandas.concat(twitter_df, ignore_index=True)\n",
        "\n",
        "# removing duplicate rows\n",
        "twitter_df.drop_duplicates(inplace=True,keep='first',ignore_index=True)\n",
        "\n",
        "#2) removing all concepts that have less than 5 examples\n",
        "twitter_df= twitter_df.groupby([1]).filter(lambda concept : len(concept)>=5)\n",
        "\n",
        "#3) Considering all examples without multiple labels for test set\n",
        "twitter_df_train= twitter_df.groupby([2]).filter(lambda phrase: len(phrase)>1)\n",
        "twitter_df_train = twitter_df_train.sample(frac=1)#this randomizes the data\n",
        "\n",
        "#dividing data into 10 parts and printing results below\n",
        "#pandas.to_csv() method can also be used to generate output in csv file\n",
        "result = numpy.array_split(twitter_df_train, 10)\n",
        "print(\"Twitter 10 fold train data shown below\")\n",
        "for r in result:\n",
        "  print(r,'\\n')\n",
        "\n",
        "# Considering all examples with multiple labels for train set\n",
        "twitter_df_test= twitter_df.groupby([2]).filter(lambda phrase: len(phrase)==1)\n",
        "twitter_df_test = twitter_df_test.sample(frac=1)\n",
        "\n",
        "#dividing data into 10 parts and printing results below.\n",
        "#pandas.to_csv() method can also be used to generate output in csv file\n",
        "result = numpy.array_split(twitter_df_test, 10)\n",
        "print(\"Twitter 10 fold test data shown below\")\n",
        "for r in result:\n",
        "  print(r,'\\n')\n",
        "\n",
        "\n",
        "#The dataset for AskAPatient\n",
        "ask_a_patient_df= []\n",
        "\n",
        "ask_a_patient_data_files= glob.glob('/content/AskAPatient/*.txt')\n",
        "\n",
        "for file in ask_a_patient_data_files:\n",
        " ask_a_patient_df.append(pandas.read_csv(file, sep='\\t',header=None, encoding='ISO-8859-1'))\n",
        "\n",
        "ask_a_patient_df=pandas.concat(ask_a_patient_df, ignore_index=True)\n",
        "\n",
        "#1) removing duplicate rows\n",
        "ask_a_patient_df.drop_duplicates(inplace=True,keep='first',ignore_index=True)\n",
        "\n",
        "#2) removing all concepets that have less than 5 examples\n",
        "ask_a_patient_df=ask_a_patient_df.groupby([1]).filter(lambda concept : len(concept)>=5)\n",
        "\n",
        "#3) considering all examples without multiple labels for test set\n",
        "ask_a_patient_df_train= ask_a_patient_df.groupby([2]).filter(lambda phrase: len(phrase)>1)\n",
        "ask_a_patient_df_train = ask_a_patient_df_train.sample(frac=1) #this randomizes the data\n",
        "\n",
        "#dividing data into 10 parts and printing below. \n",
        "#pandas.to_csv() method can also be used to generate output in csv file\n",
        "result = numpy.array_split(ask_a_patient_df_train, 10)\n",
        "print(\"AskAPatient 10 fold train data shown below\")\n",
        "for r in result:\n",
        "  print(r,'\\n')\n",
        "\n",
        "#considering all elements with multiple labels for train set\n",
        "ask_a_patient_df_test= ask_a_patient_df.groupby([2]).filter(lambda phrase: len(phrase)==1)\n",
        "ask_a_patient_df_test = ask_a_patient_df_test.sample(frac=1)\n",
        "\n",
        "#dividing data into 10 parts and printing results below. \n",
        "#pandas.to_csv() method can also be used to generate output in csv file\n",
        "result = numpy.array_split(ask_a_patient_df_test, 10)\n",
        "print(\"AskAPatient 10 fold test data shown below\")\n",
        "for r in result:\n",
        "  print(r,'\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}