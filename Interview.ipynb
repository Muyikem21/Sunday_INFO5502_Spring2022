{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interview",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRmfDrQPgXVQgcjl56chq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muyikem21/Sunday_INFO5502_Spring2022/blob/main/Interview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCOB1IsqguWj"
      },
      "outputs": [],
      "source": [
        "# Question 2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "df=pd.read_csv('/content/white_house_2017_salaries.csv')\n",
        "df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "a-k_Prv5jT5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "4UIp5ApRiUmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df['SALARY']\n",
        "df2.isnull().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "VIMPfu-aiUi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(df2)"
      ],
      "metadata": {
        "id": "HOtpSAfVjNfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(df2)"
      ],
      "metadata": {
        "id": "SAmMPecsi2Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3\n",
        "data=pd.read_csv('/content/data.csv')\n",
        "data.shape\n"
      ],
      "metadata": {
        "id": "60j_2FZtjY8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "4fUY_lSDjyi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().T"
      ],
      "metadata": {
        "id": "oSc6IxngkASX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean, median and standard deviation of data_engineer are 23.035996,23.024031 and 2.995436 respectively.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Sn8HDmnhkJxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import random\n",
        "data1 = list()\n",
        "\n",
        "for i in range(0,10000):\n",
        "    i2 = 0;\n",
        "    for x in range(0,12):\n",
        "        i2 = i2 + random()\n",
        "    data1.append(4 + 2*i2)\n",
        "        \n",
        "df2 = pd.DataFrame(data1)\n",
        "df2.to_csv(\"data.csv\")\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(data1, bins=100, range=[0,40])\n",
        "plt.title('The Histogram of Data_Engineer')"
      ],
      "metadata": {
        "id": "ILf5zGljkHaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above histogram shows that the data set is approximately normal."
      ],
      "metadata": {
        "id": "drgYbRopq0HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4\n",
        "Imputation is a technique for replacing missing data with a substitute value while retaining the majority of the dataset's data/information. These strategies are utilized because eliminating data from a dataset every time is impractical and might result in a significant reduction in the dataset's size, which not only raises issues about biasing the dataset but also leads to inaccurate analysis. We employ imputation because missing data can result in the following problems: -\n",
        "\n",
        "Yes, you read that correctly. Incompatible with the majority of Python packages used in Machine Learning. When utilizing ML libraries (the most prevalent being skLearn), they don't have a way to handle missing data automatically, which can lead to problems.\n",
        "Dataset Distortion: A large number of missing data can induce distortions in the variable distribution, causing the value of a particular category in the dataset to grow or decrease.\n",
        "Affects the Final Model: Missing data might induce a bias in the dataset, resulting in an inaccurate model analysis.\n",
        "\n",
        "Below are the tools and techniques that I would use.\n",
        "1. Complete Case Analysis(CCA):-\n",
        "This is a very simple approach of dealing with missing data, as it just removes the rows with missing data, i.e. we only evaluate rows with complete data, i.e. data is not missing. This technique is also known as \"Listwise deletion.\"\n",
        "\n",
        "Assumptions:-\n",
        "\n",
        "Randomly, data is missing (MAR).\n",
        "Data that is missing is fully erased from the table.\n",
        "Advantages:-\n",
        "\n",
        "Simple to put into practice.\n",
        "There is no need to manipulate the data.\n",
        "Limitations:-\n",
        "\n",
        "Data that has been deleted can be useful.\n",
        "Can result in the erasure of a significant amount of data.\n",
        "If a considerable volume of a certain type of variable is removed from the dataset, it can lead to bias.\n",
        "Missing data will throw the production model for a loop.\n",
        "\n",
        "When To Use It?\n",
        "\n",
        "The information is MAR (Missing At Random).\n",
        "Mixed, numerical, and categorical data work well together.\n",
        "The percentage of missing data in the dataset is between 5% and 6%.\n",
        "The dataset will not be skewed due to the lack of information in the data.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "2. Arbitrary Value Imputation\n",
        "\n",
        "Because it can handle both numerical and categorical data, this is an important technique in Imputation. This technique entails grouping missing values in a column and assigning them to a new value that is outside of the column's range. For numerical and categorical variables, we usually use values like 99999999 or -9999999, or \"Missing\" or \"Not specified.\"\n",
        "\n",
        "Assumptions:-\n",
        "\n",
        "Data does not go missing at will.\n",
        "The missing data is filled in with an arbitrary value that isn't in the dataset or the data's Mean/Median/Mode.\n",
        "Advantages:-\n",
        "\n",
        "Simple to put into practice.\n",
        "It's something we'll be able to use in production.\n",
        "If \"missing values\" exist, it preserves their significance.\n",
        "Disadvantages:-\n",
        "\n",
        "It has the potential to skew the original variable distribution.\n",
        "Outliers can be created by using arbitrary values.\n",
        "When choosing the Arbitrary value, extra caution is required.\n",
        "\n",
        "When Should You Use It?\n",
        "\n",
        "If the data isn't MAR (Missing At Random).\n",
        "Suitable for everyone.\n",
        "\n",
        "Alteryx would be my tool of choice. The Imputation tool allows the user to replace a defined value in one or more numeric data fields with a different value. Even if I was awoken early in the morning, I would use the techniques to impute the null data. There are no specific imputation tools that I would avoid.\n",
        "\n"
      ],
      "metadata": {
        "id": "cofLcZTMrYKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5\n",
        "Carl Pearson is my favorite Statistician; he is attracted to me as a result of his starling ability to navigate from one field of endeavor to another. He was a Lawyer turned Mathematician, and finally a finesse Statistician. Among his contributions to the field of Statistics are measures of correlation,and the Chi-Square distribution which is used to test the goodness of fit and association in contigency tables. His life thought me four lessons; first, there is nothing one cannot achieve. Secondly,adapting to a new envinronment. Also, challenges in a new environment can be overcome if one is focused. Lastly, one should curtivate an idea of contributing positively,and make an indelible mark in any environment one finds oneself."
      ],
      "metadata": {
        "id": "hzh7dRessABQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "F7YvddGYrN51"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}